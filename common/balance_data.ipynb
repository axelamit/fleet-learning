{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from zod import ZodFrames\n",
    "import zod.constants as constants\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_car_points(zod_frames, frame_id):\n",
    "    frame = zod_frames[frame_id]\n",
    "    # extract oxts\n",
    "    oxts = frame.oxts\n",
    "    # get timestamp\n",
    "    key_timestamp = frame.info.keyframe_time.timestamp()\n",
    "    # get posses associated with frame timestamp\n",
    "    current_pose = oxts.get_poses(key_timestamp)\n",
    "    # transform the points to the car coordinate system\n",
    "    transformed_poses = np.linalg.pinv(current_pose) @ oxts.poses\n",
    "    points = transformed_poses[:, :3, -1]\n",
    "    points = points[points[:, 0] > 0]\n",
    "    return points\n",
    "\n",
    "def euclidean_distance(coords):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between successive rows of a given array of coordinates.\n",
    "    \"\"\"\n",
    "    diffs = np.diff(coords, axis=0)\n",
    "    dists = np.sqrt(np.sum(diffs**2, axis=1))\n",
    "    return dists\n",
    "\n",
    "def get_points_at_distance(points, target_distances):\n",
    "    dists = euclidean_distance(points)\n",
    "    dists = np.insert(dists, 0, 0) # so that there is a dist for all points in points.\n",
    "    accumulated_distances = np.cumsum(dists)\n",
    "    \n",
    "    interpolated_points = np.empty((len(target_distances), points.shape[1]))\n",
    "    \n",
    "    if max(target_distances) > accumulated_distances[-1]:\n",
    "        raise ValueError(\"Target distance is larger than the accumulated distance\")\n",
    "    \n",
    "    index = 0\n",
    "    inter_idx = 0\n",
    "    for target_distance in target_distances:\n",
    "        # Increment index until we have passed the target distance\n",
    "        while accumulated_distances[index] < target_distance:\n",
    "            index += 1\n",
    "        # If we reach this state, then index - 1 is the closest index before going over.\n",
    "        # Check if the target distance is exactly at a point in the list\n",
    "        if accumulated_distances[index - 1] == target_distance:\n",
    "            interpolated_points[inter_idx] = points[index - 1]\n",
    "            inter_idx += 1\n",
    "        else:\n",
    "            # Interpolate between the two nearest points\n",
    "            p1 = points[index - 1]\n",
    "            p2 = points[index]\n",
    "            d1 = accumulated_distances[index - 1]\n",
    "            d2 = accumulated_distances[index]\n",
    "            t = (target_distance - d1) / (d2 - d1)\n",
    "            interpolated_points[inter_idx] = p1 + t * (p2 - p1)\n",
    "            inter_idx += 1\n",
    "    return interpolated_points\n",
    "\n",
    "def validate_and_categorise(zod_frames, id_set):\n",
    "    # bad data\n",
    "    too_short_ids = []\n",
    "    corrupt_ids_oxts = []\n",
    "    corrupt_ids_image = []\n",
    "\n",
    "    # categorised data\n",
    "    turns_right = []\n",
    "    turns_left = []\n",
    "    straights = []\n",
    "\n",
    "    quarter = len(id_set)//4\n",
    "\n",
    "    for frame_id in id_set:\n",
    "        # Data validation\n",
    "        # Test OXTS data and fetch it\n",
    "        corrupt = False\n",
    "        try:\n",
    "            car_points = id_to_car_points(zod_frames, frame_id)\n",
    "        except AssertionError:\n",
    "            corrupt_ids_oxts.append(frame_id)\n",
    "            corrupt = True\n",
    "        # Test if image is corrupt\n",
    "        try:\n",
    "            zod_frames[frame_id].get_image()\n",
    "        except Exception as e:\n",
    "            corrupt_ids_image.append(frame_id)\n",
    "            corrupt = True\n",
    "        if corrupt:\n",
    "            continue\n",
    "        # Test OXTS distance >= 165m\n",
    "        dists = euclidean_distance(car_points)\n",
    "        if sum(dists) < 165:\n",
    "            too_short_ids.append(frame_id)\n",
    "            continue\n",
    "        \n",
    "        # Categorise OXTS [turn-left, turn-right, straight]\n",
    "        points = id_to_car_points(zod_frames, frame_id)\n",
    "        target_distances = [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 95, 110, 125, 145, 165]\n",
    "        interpolated_points = get_points_at_distance(points, target_distances)\n",
    "        \n",
    "        threshold_in_m = 5\n",
    "        points_required = 5\n",
    "        points_to_left = interpolated_points[:,1][interpolated_points[:,1] >= threshold_in_m]\n",
    "        points_to_right = interpolated_points[:,1][interpolated_points[:,1] <= -threshold_in_m]\n",
    "        if len(points_to_right) >= points_required:\n",
    "            turns_right.append(frame_id)\n",
    "        elif len(points_to_left) >= points_required:\n",
    "            turns_left.append(frame_id)\n",
    "        else:\n",
    "            straights.append(frame_id)\n",
    "        \n",
    "    return dict(\n",
    "        too_short_ids=too_short_ids, \n",
    "        corrupt_ids_oxts=corrupt_ids_oxts, \n",
    "        corrupt_ids_image=corrupt_ids_image,\n",
    "        turns_right=turns_right,\n",
    "        turns_left=turns_left,\n",
    "        straights=straights,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc437f8de7364bb6bcd060cb29eee5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading infos: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zod_frames = ZodFrames(dataset_root=\"/mnt/ZOD\", version='full')\n",
    "training_frames_all = list(zod_frames.get_split(constants.TRAIN))\n",
    "val_frames_all = list(zod_frames.get_split(constants.VAL))\n",
    "\n",
    "# let's split this into chunks\n",
    "n_sets = 8 # number of available cpus\n",
    "# for train ids\n",
    "set_length = len(training_frames_all)//n_sets\n",
    "train_chunks = [training_frames_all[i:i + set_length] for i in range(0, len(training_frames_all), set_length)]\n",
    "# for val ids\n",
    "set_length = len(val_frames_all)//n_sets\n",
    "val_chunks = [val_frames_all[i:i + set_length] for i in range(0, len(val_frames_all), set_length)]\n",
    "\n",
    "# path = platform.data.config[\"path\"]\n",
    "# version = platform.data.config[\"version\"]\n",
    "\n",
    "# zod_frame = ZodFrames(path, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 07:23:49,104\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-06-27 07:32:50,676\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=8)\n",
    "remote_function = ray.remote(validate_and_categorise)\n",
    "train_results = ray.get([remote_function.remote(zod_frames, chunk) for chunk in train_chunks])\n",
    "ray.shutdown()\n",
    "\n",
    "ray.init(num_cpus=8)\n",
    "remote_function = ray.remote(validate_and_categorise)\n",
    "val_results = ray.get([remote_function.remote(zod_frames, chunk) for chunk in val_chunks])\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# summarise results\n",
    "def summarise(summary_dict, results):\n",
    "    for result in results:\n",
    "        for key, value in result.items():\n",
    "            summary_dict[key] += value\n",
    "    return summary_dict\n",
    "\n",
    "# write to files\n",
    "def write_summary_to_file(summary_dict, subset):\n",
    "    for key, value in summary_dict.items():\n",
    "        filename = f\"{subset}_{key}\"\n",
    "        path = Path(dir_path, filename).with_suffix(\".txt\")\n",
    "        with open(path, \"w\") as file:\n",
    "            file.write(\"\\n\".join(value))\n",
    "            \n",
    "dir_path = \"../balanced_data\"\n",
    "\n",
    "summary_dict_train = defaultdict(list)\n",
    "summary_dict_val = defaultdict(list)\n",
    "\n",
    "write_summary_to_file(summarise(summary_dict_train, train_results), \"train\")\n",
    "write_summary_to_file(summarise(summary_dict_val, val_results), \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5469 samples: val_straights.txt\n",
      "13098 samples: train_turns_right.txt\n",
      "0 samples: train_corrupt_ids_oxts.txt\n",
      "1944 samples: val_too_short_ids.txt\n",
      "0 samples: val_corrupt_ids_oxts.txt\n",
      "0 samples: val_corrupt_ids_image.txt\n",
      "47426 samples: train_straights.txt\n",
      "0 samples: train_corrupt_ids_image.txt\n",
      "1410 samples: val_turns_right.txt\n",
      "11735 samples: train_turns_left.txt\n",
      "1200 samples: val_turns_left.txt\n",
      "17713 samples: train_too_short_ids.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def filename_to_arr(filename):\n",
    "    with open(Path(dir_path, filename), \"r\") as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "dir_path = \"../balanced_data/\"\n",
    "filenames = [x for x in os.listdir(dir_path) if x.split(\"_\")[0] in [\"train\", \"val\"]]\n",
    "\n",
    "for filename in filenames:\n",
    "    content = filename_to_arr(filename)\n",
    "    print(f\"{len(content)} samples: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['val_straights.txt', 'val_turns_right.txt', 'val_turns_left.txt'],\n",
       " ['train_turns_right.txt', 'train_straights.txt', 'train_turns_left.txt'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [x for x in filenames if \"turns\" in x or \"straights\" in x]\n",
    "val = [x for x in categories if \"val\" in x]\n",
    "train = [x for x in categories if \"train\" in x]\n",
    "val, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_set(filenames):\n",
    "    subset_str = filenames[0].split(\"_\")[0]\n",
    "    id_lists = []\n",
    "    \n",
    "    # get the ids from the files\n",
    "    for filename in filenames:\n",
    "        id_lists.append(filename_to_arr(filename))\n",
    "    \n",
    "    # get the lengths of the lists\n",
    "    lens = [len(x) for x in id_lists]\n",
    "    # get the size of the smallest set\n",
    "    smallest_set_size = min(lens)\n",
    "    \n",
    "    # build the balanced dataset where each subset has the same size\n",
    "    balanced_dataset = []\n",
    "    for id_list in id_lists:\n",
    "        balanced_dataset += (random.sample(id_list, smallest_set_size))\n",
    "\n",
    "    # shuffle the data as they are appended category-wise.\n",
    "    random.shuffle(balanced_dataset)\n",
    "\n",
    "    # write to file\n",
    "    if subset_str == \"train\":\n",
    "        with open(Path(dir_path, f\"balanced_{subset_str}_ids.txt\"), \"w\") as f:\n",
    "            f.write(\"\\n\".join(balanced_dataset))\n",
    "    else:\n",
    "        with open(Path(dir_path, f\"balanced_{subset_str}_ids.txt\"), \"w\") as f:\n",
    "            f.write(\"\\n\".join(balanced_dataset))\n",
    "\n",
    "create_balanced_set(val)\n",
    "create_balanced_set(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fleet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
